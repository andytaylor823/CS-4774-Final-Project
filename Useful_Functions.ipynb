{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data and cleeeeaaaann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transform_split(fpath='data/ALL_YEARS_ADDED_FEATURES.csv',\n",
    "                         target='rate', expand=False, split=0.1, clean=True,\n",
    "                         drop_feats=['SCHOOL_YEAR','DIV_NAME','SCH_NAME','DIPLOMA_RATE'],\n",
    "                         fmt='numpy',return_pipeline=False,random_state=None):\n",
    "    '''\n",
    "    Convenience function for preparing the graduation data for machine learning!\n",
    "    \n",
    "      INPUTS:\n",
    "        fpath   - String filename of the table to load!\n",
    "        target  - Type of thing to designate y_train and y_test.\n",
    "                  Options:\n",
    "                    'DROPOUT_RATE' - y will be dropout rates\n",
    "                    'DROPOUT_N'    - y will be number of dropout students\n",
    "                    'DROPOUT'      - If expand is True, y will be 0 (graduated) or 1 (dropped out),\n",
    "                                      and table will be expanded into student-by-student rows.\n",
    "                     None           - y will not be split off. This is for unsupervised tasks like\n",
    "                                      clustering.\n",
    "        expand     - Boolean whether or not to expand tables into student-by-student rows.\n",
    "                      Default False\n",
    "        split      - Fraction of data to split off into testing set. If 0, 1, None, or False are given,\n",
    "                      data will not be split.\n",
    "        clean      - Boolean whether or not to run data through a pipeline with \n",
    "                      StandardScaler and OneHot/OrdinalEncoder.\n",
    "        drop_feats - Features to throw out.\n",
    "        fmt        - Format of the output tables. Either 'numpy' for np.ndarray outputs or 'pandas'\n",
    "                      for pandas.DataFrame outputs.\n",
    "        return_pipeline - Boolean wether or not to return the pipeline used for cleaning. If clean=False,\n",
    "                           None will be returned if return_pipeline=True.\n",
    "      OUTPUTS:\n",
    "        Some combo of the following (depending on what you ask for):\n",
    "          X_train  - Training data\n",
    "          X_test   - Testing data\n",
    "          y_train  - Training labels\n",
    "          y_test   - Testing labels\n",
    "          pipeline - Pipeline used to clean X_train and X_test.\n",
    "    '''\n",
    "    \n",
    "    ### Load ###\n",
    "    \n",
    "    df = pd.read_csv('data/ALL_YEARS_ADDED_FEATURES.csv')\n",
    "    #Get rid of any nonsense points.\n",
    "    keep = (df['DROPOUT_RATE'] >= 0) & (df['DROPOUT_RATE'] <= 100)\n",
    "    df = df[keep]\n",
    "    #Drop unwanted features.\n",
    "    if not drop_feats is None: df = df.drop(drop_feats,axis=1)\n",
    "    \n",
    "    \n",
    "    ### Transform ###\n",
    "    \n",
    "    #Parse user input for predict and expand.\n",
    "    if   target == 'DROPOUT_RATE':\n",
    "        pass #Already stored correctly\n",
    "    elif target == 'DROPOUT_N':\n",
    "        df['DROPOUT_N'] = np.round(df['COHORT_CNT']*df['DROPOUT_RATE']/100.)\n",
    "        df = df.drop(['DROPOUT_RATE'],axis=1)\n",
    "    elif target == 'DROPOUT' and expand:\n",
    "        pass #Handled in the expansion step.\n",
    "    elif target is None:\n",
    "        pass #Handled in splitting step.\n",
    "        \n",
    "    #Raise errors if input can't be used.\n",
    "    elif target == 'DROPOUT' and not expand:\n",
    "        raise ValueError(\"Cannot use boolean dropout with 'expand=False.'\"+\\\n",
    "                         \"If you are certain you want to \\nexpand rows (may take long),\"+\\\n",
    "                         \"rerun command with 'expand=True'\")\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized value of target, %s.\"%(target))\n",
    "        \n",
    "    ### Expand ###\n",
    "    # Do l8r\n",
    "    if expand:\n",
    "        pass #Add l8r\n",
    "\n",
    "    \n",
    "    ### Split ###\n",
    "    splitting = not (split is None or split==False or split==0 or split==1)\n",
    "    if not target is None: #Split X,y\n",
    "        y = df[[target]]\n",
    "        X = df.drop([target],axis=1)\n",
    "        if splitting:        #Split Train/Test\n",
    "            X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=split,random_state=random_state)\n",
    "        else:                #No Train/Test\n",
    "            X_train,y_train = X,y\n",
    "    else:                  #No X,y\n",
    "        X = df\n",
    "        if splitting:        #Split Train/Test\n",
    "            X_train,X_test = train_test_split(X,test_size=split,random_state=random_state)\n",
    "        else:                #No Train/Test\n",
    "            X_train = X\n",
    "    \n",
    "    \n",
    "    ### Pipeline ###\n",
    "    if clean:\n",
    "        pipeline = pipeline_util(X_train,clean=False,return_pipeline=True)\n",
    "        X_train = pipeline_util(X_train,pipeline=pipeline,fmt=fmt)\n",
    "        if splitting: X_test = pipeline_util(X_test, pipeline=pipeline,fmt=fmt)\n",
    "    else:\n",
    "        pipeline = None\n",
    "    \n",
    "    \n",
    "    ### Format of Output ###\n",
    "    def correct_format(Z,fmt):\n",
    "        if fmt == 'numpy':\n",
    "            if isinstance(Z,np.ndarray):\n",
    "                return Z #Already correct format\n",
    "            elif isinstance(Z,pd.DataFrame):\n",
    "                return Z.to_numpy()\n",
    "            else:\n",
    "                raise TypeError(\"Something's gone terribly wrong. Unrecognized data format, %s\"%(type(Z)))\n",
    "        elif fmt == 'pandas':\n",
    "            if isinstance(Z,np.ndarray):\n",
    "                return pd.DataFrame(Z)\n",
    "            elif isinstance(Z,pd.DataFrame):\n",
    "                return Z #Already correct format\n",
    "            else:\n",
    "                raise TypeError(\"Something's gone terribly wrong. Unrecognized data format, %s\"%(type(Z)))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid type %s, please select on of: 'numpy', 'pandas'.\"%(fmt))\n",
    "    \n",
    "    ### Return! ###\n",
    "    returns = []\n",
    "    returns.append(correct_format(X_train,fmt))\n",
    "    if splitting: returns.append(correct_format(X_test,fmt))\n",
    "    if not target is None: returns.append(correct_format(y_train,fmt))\n",
    "    if splitting and not target is None: returns.append(correct_format(y_test,fmt))\n",
    "    if return_pipeline: returns.append(pipeline)\n",
    "        \n",
    "    if len(returns) > 1:\n",
    "        return tuple(returns)\n",
    "    return returns[0]\n",
    "\n",
    "def pipeline_util(X,pipeline=None,\n",
    "                  clean=True,\n",
    "                  fmt='numpy',\n",
    "                  return_pipeline='default'):\n",
    "    '''\n",
    "    Convencience function for doing pipeline-related things.\n",
    "    \n",
    "      INPUTS:\n",
    "        X               - Data to use to create pipeline or data to clean with pipeline.\n",
    "        pipeline        - Pipeline to use on data. If none is provided, one is created.\n",
    "                           If one is provided, this function is just a pipeline runner.\n",
    "        clean           - Boolean wether or not to clean provided data. Default True.\n",
    "                           if False, this function is just a pipeline maker.\n",
    "        fmt             - Format of cleaned data.\n",
    "                           Options:\n",
    "                             'numpy' for np.ndarray output\n",
    "                             'pandas' for pandas.DataFrame output\n",
    "        return_pipeline - Boolean wether or not to return the pipeline itself.\n",
    "                           Default follows this behavior:\n",
    "                             If a pipeline is provided and clean=True, the pipeline is not returned.\n",
    "      OUTPUTS:\n",
    "        some or all of the following:\n",
    "          pipeline - The pipeline generated or provided.\n",
    "          X_clean  - X after running through the pipeline.\n",
    "    '''\n",
    "    if return_pipeline == 'default':\n",
    "        return_pipeline = not ((not pipeline is None) and clean)\n",
    "        \n",
    "    if pipeline is None:\n",
    "        pipeline = make_pipeline(X)\n",
    "        pipeline.fit(X)\n",
    "\n",
    "    if clean:\n",
    "        X_clean = pipeline.transform(X)\n",
    "        if fmt == 'numpy':\n",
    "            pass #Already numpy\n",
    "        elif fmt == 'pandas':\n",
    "            #Get column names\n",
    "            colnames = []\n",
    "            for tpl in pipeline.transformers:\n",
    "                transformer = tpl[1]\n",
    "                try:\n",
    "                    feats = transformer.get_feature_names()\n",
    "                except AttributeError:\n",
    "                    feats = tpl[2]\n",
    "                colnames.extend(feats)\n",
    "            if len(colnames) < X_clean.shape[1]:\n",
    "                colroot = colnames[-1]\n",
    "                colnames[-1] = colroot+'_0'\n",
    "                for n in range(X_clean.shape[1] - len(colnames)):\n",
    "                    colnames.append(colroot+'_%d'%(n+1))\n",
    "            X_clean = pd.DataFrame(X_clean,columns=colnames)\n",
    "    \n",
    "    #Return!\n",
    "    if return_pipeline:\n",
    "        if clean:\n",
    "            return X_clean,pipeline\n",
    "        else:\n",
    "            return pipeline\n",
    "    else:\n",
    "        if clean:\n",
    "            return X_clean\n",
    "        else:\n",
    "            return\n",
    "    \n",
    "def make_pipeline(X,cat_thresh=10):\n",
    "    '''\n",
    "    Function to make a reasonable pipeline without thinking!\n",
    "    \n",
    "    Scheme: Features are sorted based on number of unique values, N_unique.\n",
    "        If N_unique <= 2, the feature is grouped with ordinal categorical features\n",
    "        If N_unique <= cat_thresh, the feature is grouped with one-hot-encoding categorical features.\n",
    "        If N_unique >  cat_thresh, the feature is grouped with numerical feature and passed though standard_scaler.\n",
    "    \n",
    "      INPUTS:\n",
    "        X          - Data to make pipeline with.\n",
    "        cat_thresh - Threshold of unique values for something to be categorical/numerical.\n",
    "                     N_unique <= cat_thresh -> categorical, else numerical.\n",
    "      OUTPUTS:\n",
    "        pipeline - The pipeline!\n",
    "    '''\n",
    "    #Get all features.\n",
    "    all_feats = X.columns\n",
    "    \n",
    "    #Go through features and determine which type it is.\n",
    "    num_feats = []\n",
    "    ord_cat_feats = []\n",
    "    ohe_cat_feats = []\n",
    "    for feat in all_feats:\n",
    "        unique = pd.unique(X[feat])\n",
    "        if len(unique) <= 2:\n",
    "            ord_cat_feats.append(feat)\n",
    "        elif len(unique) <= cat_thresh:\n",
    "            ohe_cat_feats.append(feat)\n",
    "        else:\n",
    "            num_feats.append(feat)\n",
    "    \n",
    "    #Make the pipeline and return!\n",
    "    pipeline = ColumnTransformer([\n",
    "        ('num',StandardScaler(),num_feats),\n",
    "        ('ord',OrdinalEncoder(),ord_cat_feats),\n",
    "        ('ohe',OneHotEncoder(categories='auto'),ohe_cat_feats),\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57656, 19) (14414, 19)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = load_transform_split(target='DROPOUT_RATE',expand=False,clean=True,split=0.2)\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57656, 20) (14414, 20)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test = load_transform_split(target=None,expand=False,clean=True,split=0.2)\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72070, 19) (72070, 1)\n"
     ]
    }
   ],
   "source": [
    "X,y = load_transform_split(target='DROPOUT_N',expand=False,clean=True,split=None)\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72070, 20)\n"
     ]
    }
   ],
   "source": [
    "X = load_transform_split(target=None,expand=False,clean=True,split=None)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
