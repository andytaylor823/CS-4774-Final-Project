{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import useful_functions as uf\n",
    "\n",
    "#load_transform_split(fpath='data/ALL_YEARS_ADDED_FEATURES.csv',\n",
    "#                         target='DROPOUT_N', expand=False, split=0.1, clean=True,\n",
    "#                         drop_feats=['SCHOOL_YEAR','DIV_NAME','SCH_NAME','DIPLOMA_RATE'],\n",
    "#                         fmt='numpy',return_pipeline=False):\n",
    "\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = uf.load_transform_split(target='DROPOUT_N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables about whether or not to modify the hyperparameters\n",
    "USE_BATCH_NORMALIZATION = True\n",
    "USE_ADAM_OPTIMIZER = True\n",
    "USE_DROPOUT_REGULARIZATION = True\n",
    "USE_SPECIAL_INITIALIZATION = True\n",
    "USE_LR_SCHEDULER = True\n",
    "\n",
    "# some of the modifications to the hyperparams -- don't really expect these to change\n",
    "ACTIVATOR = 'selu' # the one suggested by slides 21 & 44 on Lecture 11\n",
    "INITIALIZER = 'he_normal' # the one suggested by slides 17 & 44 on Lecture 11\n",
    "\n",
    "# some more modifications to the hyperparams -- probably change these later on\n",
    "# for now, set them to the default values (except the LR -- upped by a factor of 10)\n",
    "DROPOUT_RATE = 0.2\n",
    "ADAM_LR = 0.01\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "NUM_NODES = 10\n",
    "N_EPOCHS = 30\n",
    "LR_10FOLD_DECAY_TIME = 20 # decay by a factor of 10 every X epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def initialize_model(use_drop=USE_DROPOUT_REGULARIZATION, use_batch=USE_BATCH_NORMALIZATION,\n",
    "                    drop_rate=DROPOUT_RATE, num_nodes=NUM_NODES,\n",
    "                    use_init=USE_SPECIAL_INITIALIZATION, init=INITIALIZER, activ=ACTIVATOR):\n",
    "    model = keras.models.Sequential()\n",
    "    if use_drop:\n",
    "        model.add(keras.layers.Dropout(rate=drop_rate))\n",
    "    if use_init:\n",
    "        model.add(keras.layers.Dense(19, activation=activ, input_shape=X_train_scaled.shape[1:],\n",
    "                                     kernel_initializer=init))\n",
    "    else:\n",
    "        model.add(keras.layers.Dense(19, activation=activ, input_shape=X_train_scaled.shape[1:]))\n",
    "    if use_batch:\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        \n",
    "        \n",
    "    for i in np.arange(19.0, 1, NUM_NODES/(-18)):\n",
    "        j = int(i)\n",
    "        if use_drop:\n",
    "            model.add(keras.layers.Dropout(rate=drop_rate))\n",
    "        if use_init:\n",
    "            model.add(keras.layers.Dense(j, activation=activ, kernel_initializer=init))\n",
    "        else:\n",
    "            model.add(keras.layers.Dense(j, activation=activ))\n",
    "        if use_batch:\n",
    "            model.add(keras.layers.BatchNormalization())\n",
    "    if use_init:\n",
    "        model.add(keras.layers.Dense(1, kernel_initializer=init))\n",
    "    else:\n",
    "        model.add(keras.layers.Dense(1))\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, use_adam=USE_ADAM_OPTIMIZER, lr=ADAM_LR, b1=ADAM_BETA1, b2=ADAM_BETA2):\n",
    "    if use_adam:\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr, beta_1=b1, beta_2=b2)\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    else:\n",
    "        model.compile(loss='mean_squared_error')\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, use_sched=USE_LR_SCHEDULER, decay_time=LR_10FOLD_DECAY_TIME, init=ADAM_LR,\n",
    "              use_adam=USE_ADAM_OPTIMIZER, n_epo=N_EPOCHS):\n",
    "    if not use_adam or not use_sched:\n",
    "        history=model.fit(X_train_scaled, y_train, epochs=n_epo)\n",
    "    else:\n",
    "        def lr_fn(epoch): return(init * 10**(-epoch/decay_time))\n",
    "        lr_scheduler = keras.callbacks.LearningRateScheduler(lr_fn)\n",
    "        history = model.fit(X_train_scaled, y_train, epochs=n_epo, callbacks=[lr_scheduler])\n",
    "    return(history, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def analyze_performance(history, model):\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print('RMSE: %.2f' %rmse)\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can I just do this use *args or something? IDK\n",
    "def run(use_drop=USE_DROPOUT_REGULARIZATION, use_batch=USE_BATCH_NORMALIZATION,\n",
    "        drop_rate=DROPOUT_RATE, num_nodes=NUM_NODES,\n",
    "        use_init=USE_SPECIAL_INITIALIZATION, init=INITIALIZER, activ=ACTIVATOR,\n",
    "        use_adam=USE_ADAM_OPTIMIZER, lr=ADAM_LR, b1=ADAM_BETA1, b2=ADAM_BETA2,\n",
    "        use_sched=USE_LR_SCHEDULER, decay_time=LR_10FOLD_DECAY_TIME, n_epo=N_EPOCHS):\n",
    "    \n",
    "    model = initialize_model(use_drop, use_batch, drop_rate, num_nodes, use_init, init, activ)\n",
    "    model = compile_model(model, use_adam, lr, b1, b2)\n",
    "    history, model = fit_model(model, use_sched, decay_time, lr, use_adam, n_epo)\n",
    "    rmse = analyze_performance(history, model)\n",
    "    return(history, model, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64863 samples\n",
      "Epoch 1/30\n",
      "64863/64863 [==============================] - 173s 3ms/sample - loss: 4.5575\n",
      "Epoch 2/30\n",
      "64863/64863 [==============================] - 122s 2ms/sample - loss: 4.5307\n",
      "Epoch 3/30\n",
      "64863/64863 [==============================] - 132s 2ms/sample - loss: 4.5300\n",
      "Epoch 4/30\n",
      "64863/64863 [==============================] - 120s 2ms/sample - loss: 4.5295\n",
      "Epoch 5/30\n",
      "64863/64863 [==============================] - 118s 2ms/sample - loss: 4.5290\n",
      "Epoch 6/30\n",
      "64863/64863 [==============================] - 119s 2ms/sample - loss: 4.5287\n",
      "Epoch 7/30\n",
      "64863/64863 [==============================] - 131s 2ms/sample - loss: 4.5289\n",
      "Epoch 8/30\n",
      "64863/64863 [==============================] - 121s 2ms/sample - loss: 4.5290\n",
      "Epoch 9/30\n",
      "64863/64863 [==============================] - 120s 2ms/sample - loss: 4.5284\n",
      "Epoch 10/30\n",
      "64863/64863 [==============================] - 122s 2ms/sample - loss: 4.5284\n",
      "Epoch 11/30\n",
      "64863/64863 [==============================] - 126s 2ms/sample - loss: 4.5282\n",
      "Epoch 12/30\n",
      "64863/64863 [==============================] - 135s 2ms/sample - loss: 4.5280\n",
      "Epoch 13/30\n",
      "64863/64863 [==============================] - 119s 2ms/sample - loss: 4.5281\n",
      "Epoch 14/30\n",
      "64863/64863 [==============================] - 121s 2ms/sample - loss: 4.5280\n",
      "Epoch 15/30\n",
      "64863/64863 [==============================] - 125s 2ms/sample - loss: 4.5278\n",
      "Epoch 16/30\n",
      "64863/64863 [==============================] - 120s 2ms/sample - loss: 4.5280\n",
      "Epoch 17/30\n",
      "64863/64863 [==============================] - 131s 2ms/sample - loss: 4.5276\n",
      "Epoch 18/30\n",
      "64863/64863 [==============================] - 120s 2ms/sample - loss: 4.5267\n",
      "Epoch 19/30\n",
      "64863/64863 [==============================] - 126s 2ms/sample - loss: 4.5250\n",
      "Epoch 20/30\n",
      "64863/64863 [==============================] - 125s 2ms/sample - loss: 4.5220\n",
      "Epoch 21/30\n",
      "64863/64863 [==============================] - 131s 2ms/sample - loss: 4.5101\n",
      "Epoch 22/30\n",
      "64863/64863 [==============================] - 125s 2ms/sample - loss: 4.4888\n",
      "Epoch 23/30\n",
      "64863/64863 [==============================] - 119s 2ms/sample - loss: 4.4733\n",
      "Epoch 24/30\n",
      "64863/64863 [==============================] - 119s 2ms/sample - loss: 4.4601\n",
      "Epoch 25/30\n",
      "64863/64863 [==============================] - 125s 2ms/sample - loss: 4.4364\n",
      "Epoch 26/30\n",
      "64863/64863 [==============================] - 125s 2ms/sample - loss: 4.4184\n",
      "Epoch 27/30\n",
      "64863/64863 [==============================] - 199s 3ms/sample - loss: 4.4211\n",
      "Epoch 28/30\n",
      "64863/64863 [==============================] - 195s 3ms/sample - loss: 4.4083\n",
      "Epoch 29/30\n",
      "64863/64863 [==============================] - 123s 2ms/sample - loss: 4.4072\n",
      "Epoch 30/30\n",
      "64863/64863 [==============================] - 120s 2ms/sample - loss: 4.4037\n",
      "RMSE: 2.32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.callbacks.History at 0x1a4bba6f90>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x10d07eb10>,\n",
       " 2.323650147174013)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2203a517bd9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# defaults are lr = 0.001, beta1 = 0.9, beta2 = 0.999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mADAM_LR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mADAM_BETA1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mADAM_BETA2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# adjust the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# beta(1,2) correspond to the importance of (momentum, scale(?))\n",
    "# defaults are lr = 0.001, beta1 = 0.9, beta2 = 0.999\n",
    "optimizer = keras.optimizers.Adam(learning_rate=ADAM_LR, beta_1=ADAM_BETA1, beta_2=ADAM_BETA2)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "# adjust the learning rate\n",
    "def lr_fn(epoch, init=ADAM_LR, decay_time=LR_10FOLD_DECAY_TIME):\n",
    "    return(init * 10**(-epoch/decay_time))\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(lr_fn)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=N_EPOCHS, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "if USE_DROPOUT_REGULARIZATION:\n",
    "    model.add(keras.layers.Dropout(rate=DROPOUT_RATE, input_shape=X_train_scaled.shape[1:]))\n",
    "model.add(keras.layers.Dense(19, activation=ACTIVATOR, kernel_initializer=INITIALIZER))\n",
    "if USE_BATCH_NORMALIZATION:\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "for i in np.arange(19, 1, -18/NUM_NODES): # always start with 19 and end before 1\n",
    "    j = int(i)\n",
    "    model.add(keras.layers.Dropout(rate=DROPOUT_RATE))\n",
    "    model.add(keras.layers.Dense(j, activation=ACTIVATOR, kernel_initializer=INITIALIZER))\n",
    "    if USE_BATCH_NORMALIZATION:\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(1, kernel_initializer=INITIALIZER)) # the final, predicting layer\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(keras.optimizers.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(1, 10, 1.5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(19, 1, -18/NUM_NODES):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
