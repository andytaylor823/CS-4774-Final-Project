{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import useful_functions as uf\n",
    "\n",
    "#load_transform_split(fpath='data/ALL_YEARS_ADDED_FEATURES.csv',\n",
    "#                         target='DROPOUT_N', expand=False, split=0.1, clean=True,\n",
    "#                         drop_feats=['SCHOOL_YEAR','DIV_NAME','SCH_NAME','DIPLOMA_RATE'],\n",
    "#                         fmt='numpy',return_pipeline=False):\n",
    "\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = uf.load_transform_split(target='DROPOUT_N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables about whether or not to modify the hyperparameters\n",
    "USE_BATCH_NORMALIZATION = False\n",
    "USE_ADAM_OPTIMIZER = True\n",
    "USE_DROPOUT_REGULARIZATION = False\n",
    "USE_SPECIAL_INITIALIZATION = True\n",
    "USE_LR_SCHEDULER = True\n",
    "\n",
    "# some of the modifications to the hyperparams -- don't really expect these to change\n",
    "ACTIVATOR = 'selu' # the one suggested by slides 21 & 44 on Lecture 11\n",
    "INITIALIZER = 'he_normal' # the one suggested by slides 17 & 44 on Lecture 11\n",
    "\n",
    "# some more modifications to the hyperparams -- probably change these later on\n",
    "# for now, set them to the default values\n",
    "DROPOUT_RATE = 0.0\n",
    "ADAM_LR = 0.001\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "NUM_LAYERS = 10\n",
    "N_EPOCHS = 30\n",
    "USE_EARLY_STOPPING = True\n",
    "LR_10FOLD_DECAY_TIME = 20 # decay by a factor of 10 every X epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def initialize_model(use_drop=USE_DROPOUT_REGULARIZATION, use_batch=USE_BATCH_NORMALIZATION,\n",
    "                    drop_rate=DROPOUT_RATE, num_layers=NUM_LAYERS,\n",
    "                    use_init=USE_SPECIAL_INITIALIZATION, init=INITIALIZER, activ=ACTIVATOR):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=X_train_scaled.shape[1:]))\n",
    "        \n",
    "    for i in np.arange(19.0, 1, num_layers/(-18)):\n",
    "        j = int(i)\n",
    "        if use_drop:\n",
    "            model.add(keras.layers.Dropout(rate=drop_rate))\n",
    "        if use_init:\n",
    "            model.add(keras.layers.Dense(j, activation=activ, kernel_initializer=init))\n",
    "        else:\n",
    "            model.add(keras.layers.Dense(j, activation=activ))\n",
    "        if use_batch:\n",
    "            model.add(keras.layers.BatchNormalization())\n",
    "    if use_init:\n",
    "        model.add(keras.layers.Dense(1, kernel_initializer=init))\n",
    "    else:\n",
    "        model.add(keras.layers.Dense(1))\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, use_adam=USE_ADAM_OPTIMIZER, lr=ADAM_LR, b1=ADAM_BETA1, b2=ADAM_BETA2):\n",
    "    if use_adam:\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr, beta_1=b1, beta_2=b2)\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    else:\n",
    "        model.compile(loss='mean_squared_error')\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, use_sched=USE_LR_SCHEDULER, decay_time=LR_10FOLD_DECAY_TIME, init=ADAM_LR,\n",
    "              use_adam=USE_ADAM_OPTIMIZER, n_epo=N_EPOCHS, stop_early=USE_EARLY_STOPPING):\n",
    "    if not use_adam or not use_sched:\n",
    "        history=model.fit(X_train_scaled, y_train, epochs=n_epo)\n",
    "    else:\n",
    "        callbacks = []\n",
    "        if use_sched:\n",
    "            def lr_fn(epoch): return(init * 10**(-epoch/decay_time))\n",
    "            callbacks.append(keras.callbacks.LearningRateScheduler(lr_fn))\n",
    "        if stop_early:\n",
    "            callbacks.append(keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True))\n",
    "        history = model.fit(X_train_scaled, y_train, epochs=n_epo, callbacks=callbacks, validation_split=0.2)\n",
    "    return(history, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def analyze_performance(history, model):\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print('RMSE: %.2f' %rmse)\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can I just do this use *args or something? IDK\n",
    "def run(use_drop=USE_DROPOUT_REGULARIZATION, use_batch=USE_BATCH_NORMALIZATION,\n",
    "        drop_rate=DROPOUT_RATE, num_layers=NUM_LAYERS,\n",
    "        use_init=USE_SPECIAL_INITIALIZATION, init=INITIALIZER, activ=ACTIVATOR,\n",
    "        use_adam=USE_ADAM_OPTIMIZER, lr=ADAM_LR, b1=ADAM_BETA1, b2=ADAM_BETA2,\n",
    "        use_sched=USE_LR_SCHEDULER, decay_time=LR_10FOLD_DECAY_TIME, n_epo=N_EPOCHS, stop_early=USE_EARLY_STOPPING):\n",
    "    \n",
    "    model = initialize_model(use_drop, use_batch, drop_rate, num_layers, use_init, init, activ)\n",
    "    model = compile_model(model, use_adam, lr, b1, b2)\n",
    "    history, model = fit_model(model, use_sched, decay_time, lr, use_adam, n_epo, stop_early)\n",
    "    rmse = analyze_performance(history, model)\n",
    "    return(history, model, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51890 samples, validate on 12973 samples\n",
      "Epoch 1/100\n",
      "51890/51890 [==============================] - 8s 155us/sample - loss: 4.3669 - val_loss: 5.0416\n",
      "Epoch 2/100\n",
      "51890/51890 [==============================] - 6s 125us/sample - loss: 3.6114 - val_loss: 4.6048\n",
      "Epoch 3/100\n",
      "51890/51890 [==============================] - 10s 189us/sample - loss: 3.2189 - val_loss: 4.4443\n",
      "Epoch 4/100\n",
      "51890/51890 [==============================] - 9s 170us/sample - loss: 3.0196 - val_loss: 4.0984\n",
      "Epoch 5/100\n",
      "51890/51890 [==============================] - 15s 286us/sample - loss: 2.8364 - val_loss: 3.8400\n",
      "Epoch 6/100\n",
      "51890/51890 [==============================] - 16s 317us/sample - loss: 2.9077 - val_loss: 3.9872\n",
      "Epoch 7/100\n",
      "51890/51890 [==============================] - 11s 204us/sample - loss: 3.0461 - val_loss: 4.7686\n",
      "Epoch 8/100\n",
      "51890/51890 [==============================] - 8s 149us/sample - loss: 3.1136 - val_loss: 4.1222\n",
      "Epoch 9/100\n",
      "51890/51890 [==============================] - 8s 145us/sample - loss: 2.8886 - val_loss: 3.9153\n",
      "Epoch 10/100\n",
      "51890/51890 [==============================] - 7s 143us/sample - loss: 2.7862 - val_loss: 3.9018\n",
      "Epoch 11/100\n",
      "51890/51890 [==============================] - 8s 162us/sample - loss: 2.8085 - val_loss: 3.9893\n",
      "Epoch 12/100\n",
      "51890/51890 [==============================] - 8s 145us/sample - loss: 2.8459 - val_loss: 3.8922\n",
      "Epoch 13/100\n",
      "51890/51890 [==============================] - 7s 140us/sample - loss: 3.0144 - val_loss: 3.9370\n",
      "Epoch 14/100\n",
      "51890/51890 [==============================] - 7s 130us/sample - loss: 2.7916 - val_loss: 3.8850\n",
      "Epoch 15/100\n",
      "51890/51890 [==============================] - 7s 129us/sample - loss: 2.7000 - val_loss: 3.8003\n",
      "Epoch 16/100\n",
      "51890/51890 [==============================] - 7s 128us/sample - loss: 2.6781 - val_loss: 3.7140\n",
      "Epoch 17/100\n",
      "51890/51890 [==============================] - 7s 130us/sample - loss: 2.6987 - val_loss: 3.7269\n",
      "Epoch 18/100\n",
      "51890/51890 [==============================] - 7s 133us/sample - loss: 2.6502 - val_loss: 3.8438\n",
      "Epoch 19/100\n",
      "51890/51890 [==============================] - 7s 128us/sample - loss: 2.6101 - val_loss: 3.6407\n",
      "Epoch 20/100\n",
      "51890/51890 [==============================] - 7s 127us/sample - loss: 2.5472 - val_loss: 3.6389\n",
      "Epoch 21/100\n",
      "51890/51890 [==============================] - 7s 128us/sample - loss: 2.4944 - val_loss: 3.5237\n",
      "Epoch 22/100\n",
      "51890/51890 [==============================] - 7s 131us/sample - loss: 2.4541 - val_loss: 3.5126\n",
      "Epoch 23/100\n",
      "51890/51890 [==============================] - 7s 126us/sample - loss: 2.4358 - val_loss: 3.5213\n",
      "Epoch 24/100\n",
      "51890/51890 [==============================] - 7s 130us/sample - loss: 2.4121 - val_loss: 3.4077\n",
      "Epoch 25/100\n",
      "51890/51890 [==============================] - 8s 145us/sample - loss: 2.3939 - val_loss: 3.4337\n",
      "Epoch 26/100\n",
      "51890/51890 [==============================] - 7s 133us/sample - loss: 2.4132 - val_loss: 3.4401\n",
      "Epoch 27/100\n",
      "51890/51890 [==============================] - 7s 130us/sample - loss: 2.3922 - val_loss: 3.4607\n",
      "Epoch 28/100\n",
      "51890/51890 [==============================] - 7s 132us/sample - loss: 2.3829 - val_loss: 3.4007\n",
      "Epoch 29/100\n",
      "51890/51890 [==============================] - 7s 130us/sample - loss: 2.3659 - val_loss: 3.3805\n",
      "Epoch 30/100\n",
      "51890/51890 [==============================] - 7s 133us/sample - loss: 2.3500 - val_loss: 3.3594\n",
      "Epoch 31/100\n",
      "51890/51890 [==============================] - 7s 132us/sample - loss: 2.3514 - val_loss: 3.3950\n",
      "Epoch 32/100\n",
      "51890/51890 [==============================] - 7s 131us/sample - loss: 2.3598 - val_loss: 3.3798\n",
      "Epoch 33/100\n",
      "51890/51890 [==============================] - 7s 132us/sample - loss: 2.3404 - val_loss: 3.3690\n",
      "Epoch 34/100\n",
      "51890/51890 [==============================] - 7s 138us/sample - loss: 2.3306 - val_loss: 3.3496\n",
      "Epoch 35/100\n",
      "51890/51890 [==============================] - 7s 131us/sample - loss: 2.3131 - val_loss: 3.3120\n",
      "Epoch 36/100\n",
      "51890/51890 [==============================] - 7s 132us/sample - loss: 2.3099 - val_loss: 3.3315\n",
      "Epoch 37/100\n",
      "51890/51890 [==============================] - 7s 130us/sample - loss: 2.3095 - val_loss: 3.3291\n",
      "Epoch 38/100\n",
      "51890/51890 [==============================] - 7s 127us/sample - loss: 2.3100 - val_loss: 3.3264\n",
      "Epoch 39/100\n",
      "51890/51890 [==============================] - 7s 132us/sample - loss: 2.3045 - val_loss: 3.3295\n",
      "Epoch 40/100\n",
      "51890/51890 [==============================] - 7s 140us/sample - loss: 2.2927 - val_loss: 3.3203\n",
      "Epoch 41/100\n",
      "51890/51890 [==============================] - 9s 166us/sample - loss: 2.2938 - val_loss: 3.3219\n",
      "Epoch 42/100\n",
      "51890/51890 [==============================] - 8s 149us/sample - loss: 2.2835 - val_loss: 3.3109\n",
      "Epoch 43/100\n",
      "51890/51890 [==============================] - 7s 140us/sample - loss: 2.2760 - val_loss: 3.3269\n",
      "Epoch 44/100\n",
      "51890/51890 [==============================] - 7s 138us/sample - loss: 2.2736 - val_loss: 3.2947\n",
      "Epoch 45/100\n",
      "51890/51890 [==============================] - 8s 145us/sample - loss: 2.2633 - val_loss: 3.2847\n",
      "Epoch 46/100\n",
      "51890/51890 [==============================] - 9s 166us/sample - loss: 2.2599 - val_loss: 3.2765\n",
      "Epoch 47/100\n",
      "51890/51890 [==============================] - 7s 140us/sample - loss: 2.2547 - val_loss: 3.2654\n",
      "Epoch 48/100\n",
      "51890/51890 [==============================] - 8s 155us/sample - loss: 2.2538 - val_loss: 3.2620\n",
      "Epoch 49/100\n",
      "51890/51890 [==============================] - 8s 150us/sample - loss: 2.2544 - val_loss: 3.2854\n",
      "Epoch 50/100\n",
      "51890/51890 [==============================] - 8s 154us/sample - loss: 2.2511 - val_loss: 3.2694\n",
      "Epoch 51/100\n",
      "51890/51890 [==============================] - 7s 139us/sample - loss: 2.2473 - val_loss: 3.2620\n",
      "Epoch 52/100\n",
      "51890/51890 [==============================] - 7s 136us/sample - loss: 2.2436 - val_loss: 3.2675\n",
      "Epoch 53/100\n",
      "51890/51890 [==============================] - 7s 132us/sample - loss: 2.2410 - val_loss: 3.2504\n",
      "Epoch 54/100\n",
      "51890/51890 [==============================] - 7s 131us/sample - loss: 2.2381 - val_loss: 3.2529\n",
      "Epoch 55/100\n",
      "51890/51890 [==============================] - 7s 137us/sample - loss: 2.2349 - val_loss: 3.2476\n",
      "Epoch 56/100\n",
      "51890/51890 [==============================] - 7s 131us/sample - loss: 2.2331 - val_loss: 3.2417\n",
      "Epoch 57/100\n",
      "51890/51890 [==============================] - 7s 131us/sample - loss: 2.2306 - val_loss: 3.2425\n",
      "Epoch 58/100\n",
      "51890/51890 [==============================] - 7s 139us/sample - loss: 2.2312 - val_loss: 3.2456\n",
      "Epoch 59/100\n",
      "51890/51890 [==============================] - 6s 121us/sample - loss: 2.2313 - val_loss: 3.2385\n",
      "Epoch 60/100\n",
      "51890/51890 [==============================] - 7s 135us/sample - loss: 2.2275 - val_loss: 3.2372\n",
      "Epoch 61/100\n",
      "51890/51890 [==============================] - 7s 136us/sample - loss: 2.2269 - val_loss: 3.2356\n",
      "Epoch 62/100\n",
      "51890/51890 [==============================] - 8s 149us/sample - loss: 2.2231 - val_loss: 3.2407\n",
      "Epoch 63/100\n",
      "51890/51890 [==============================] - 9s 170us/sample - loss: 2.2236 - val_loss: 3.2411\n",
      "Epoch 64/100\n",
      "51890/51890 [==============================] - 8s 163us/sample - loss: 2.2214 - val_loss: 3.2392\n",
      "Epoch 65/100\n",
      "51890/51890 [==============================] - 7s 128us/sample - loss: 2.2221 - val_loss: 3.2436\n",
      "Epoch 66/100\n",
      "51890/51890 [==============================] - 7s 126us/sample - loss: 2.2179 - val_loss: 3.2340\n",
      "Epoch 67/100\n",
      "51890/51890 [==============================] - 7s 125us/sample - loss: 2.2165 - val_loss: 3.2363\n",
      "Epoch 68/100\n",
      "51890/51890 [==============================] - 6s 121us/sample - loss: 2.2186 - val_loss: 3.2301\n",
      "Epoch 69/100\n",
      "51890/51890 [==============================] - 6s 120us/sample - loss: 2.2175 - val_loss: 3.2359\n",
      "Epoch 70/100\n",
      "51890/51890 [==============================] - 6s 121us/sample - loss: 2.2172 - val_loss: 3.2327\n",
      "Epoch 71/100\n",
      "51890/51890 [==============================] - 6s 121us/sample - loss: 2.2166 - val_loss: 3.2284\n",
      "Epoch 72/100\n",
      "51890/51890 [==============================] - 6s 123us/sample - loss: 2.2133 - val_loss: 3.2271\n",
      "Epoch 73/100\n",
      "51890/51890 [==============================] - 6s 123us/sample - loss: 2.2135 - val_loss: 3.2279\n",
      "Epoch 74/100\n",
      "51890/51890 [==============================] - 10s 200us/sample - loss: 2.2119 - val_loss: 3.2332\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51890/51890 [==============================] - 7s 141us/sample - loss: 2.2120 - val_loss: 3.2290\n",
      "Epoch 76/100\n",
      "51890/51890 [==============================] - 8s 149us/sample - loss: 2.2117 - val_loss: 3.2359\n",
      "Epoch 77/100\n",
      "51890/51890 [==============================] - 6s 118us/sample - loss: 2.2114 - val_loss: 3.2311\n",
      "Epoch 78/100\n",
      "51890/51890 [==============================] - 6s 121us/sample - loss: 2.2104 - val_loss: 3.2291\n",
      "Epoch 79/100\n",
      "51890/51890 [==============================] - 7s 127us/sample - loss: 2.2096 - val_loss: 3.2289\n",
      "Epoch 80/100\n",
      "51890/51890 [==============================] - 8s 149us/sample - loss: 2.2095 - val_loss: 3.2284\n",
      "Epoch 81/100\n",
      "51890/51890 [==============================] - 8s 146us/sample - loss: 2.2094 - val_loss: 3.2276\n",
      "Epoch 82/100\n",
      "51890/51890 [==============================] - 7s 135us/sample - loss: 2.2087 - val_loss: 3.2289\n",
      "RMSE: 1.58\n"
     ]
    }
   ],
   "source": [
    "history, model, rmse = run(n_epo=100, decay_time=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64863 samples\n",
      "Epoch 1/30\n",
      "64863/64863 [==============================] - 157s 2ms/sample - loss: 4.5513\n",
      "Epoch 2/30\n",
      "64863/64863 [==============================] - 126s 2ms/sample - loss: 4.5492\n",
      "Epoch 3/30\n",
      "64863/64863 [==============================] - 119s 2ms/sample - loss: 4.5433\n",
      "Epoch 4/30\n",
      "64863/64863 [==============================] - 120s 2ms/sample - loss: 4.5408\n",
      "Epoch 5/30\n",
      "64863/64863 [==============================] - 121s 2ms/sample - loss: 4.5404\n",
      "Epoch 6/30\n",
      "64863/64863 [==============================] - 121s 2ms/sample - loss: 4.5415\n",
      "Epoch 7/30\n",
      "64863/64863 [==============================] - 121s 2ms/sample - loss: 4.5408\n",
      "Epoch 8/30\n",
      "64863/64863 [==============================] - 130s 2ms/sample - loss: 4.5336\n",
      "Epoch 9/30\n",
      "64863/64863 [==============================] - 126s 2ms/sample - loss: 4.5368\n",
      "Epoch 10/30\n",
      "64863/64863 [==============================] - 121s 2ms/sample - loss: 4.5354\n",
      "Epoch 11/30\n",
      "64863/64863 [==============================] - 120s 2ms/sample - loss: 4.5336\n",
      "Epoch 12/30\n",
      "64863/64863 [==============================] - 126s 2ms/sample - loss: 4.5333\n",
      "Epoch 13/30\n",
      "64863/64863 [==============================] - 131s 2ms/sample - loss: 4.5334\n",
      "Epoch 14/30\n",
      "18688/64863 [=======>......................] - ETA: 1:36 - loss: 4.7846"
     ]
    }
   ],
   "source": [
    "# learn faster -- the loss was decreasing pretty slowly before\n",
    "history, model, rmse = run(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [4.366924184796254,\n",
       "  3.61136914084334,\n",
       "  3.2188835397417273,\n",
       "  3.019615872964438,\n",
       "  2.8364456270213854,\n",
       "  2.9077159593734327,\n",
       "  3.0460632276250164,\n",
       "  3.1136287702625487,\n",
       "  2.888588472767174,\n",
       "  2.786225790484927,\n",
       "  2.808483366002281,\n",
       "  2.8458719423566623,\n",
       "  3.0144425138110345,\n",
       "  2.791646230002547,\n",
       "  2.700002121865623,\n",
       "  2.678068341352412,\n",
       "  2.698746129638957,\n",
       "  2.6502302295051114,\n",
       "  2.610062224236865,\n",
       "  2.547221835492594,\n",
       "  2.4944142985559883,\n",
       "  2.454124903095368,\n",
       "  2.4358208498804985,\n",
       "  2.4121204198681796,\n",
       "  2.393923916897578,\n",
       "  2.4132180872990876,\n",
       "  2.3922308584104086,\n",
       "  2.3828636112436437,\n",
       "  2.3658543868799113,\n",
       "  2.350007328129752,\n",
       "  2.3513827438241,\n",
       "  2.35981289684761,\n",
       "  2.34040990737528,\n",
       "  2.3306415417815107,\n",
       "  2.3130823144657255,\n",
       "  2.309947010464098,\n",
       "  2.3094986305780827,\n",
       "  2.309954740459415,\n",
       "  2.304480988183539,\n",
       "  2.292706018221082,\n",
       "  2.293803950269504,\n",
       "  2.2835321007792166,\n",
       "  2.27595340010058,\n",
       "  2.273564491972204,\n",
       "  2.263337734141178,\n",
       "  2.259856311460294,\n",
       "  2.2547418138383097,\n",
       "  2.2537618084044833,\n",
       "  2.25438026724856,\n",
       "  2.2511320380581945,\n",
       "  2.247299500423995,\n",
       "  2.2435775002387475,\n",
       "  2.240952287507806,\n",
       "  2.238144915846874,\n",
       "  2.234868681313652,\n",
       "  2.2331073536489665,\n",
       "  2.2305995771066605,\n",
       "  2.2311894012556572,\n",
       "  2.2312640370695918,\n",
       "  2.227461623621713,\n",
       "  2.226868795220575,\n",
       "  2.223055128298598,\n",
       "  2.223631378413855,\n",
       "  2.2214348043451695,\n",
       "  2.222122175731648,\n",
       "  2.2178573769212666,\n",
       "  2.216488927672837,\n",
       "  2.218580734313947,\n",
       "  2.217473798984506,\n",
       "  2.217199402327344,\n",
       "  2.216571278814682,\n",
       "  2.213297886291573,\n",
       "  2.2135037287808403,\n",
       "  2.2119457136943725,\n",
       "  2.211975071484883,\n",
       "  2.2116665113452405,\n",
       "  2.2114390601310503,\n",
       "  2.2103759360579907,\n",
       "  2.20964085008855,\n",
       "  2.209516813038723,\n",
       "  2.209380592654771,\n",
       "  2.2086998392286317],\n",
       " 'val_loss': [5.041604193589521,\n",
       "  4.604815824493303,\n",
       "  4.444279396303322,\n",
       "  4.098355782902214,\n",
       "  3.839964556336118,\n",
       "  3.987228661924911,\n",
       "  4.768550682700076,\n",
       "  4.122201061506991,\n",
       "  3.9153398947759683,\n",
       "  3.9018012525594603,\n",
       "  3.989315564034394,\n",
       "  3.8921924266149532,\n",
       "  3.9369677561549556,\n",
       "  3.8849693580463747,\n",
       "  3.8002993166883163,\n",
       "  3.7140016278782815,\n",
       "  3.72686693143534,\n",
       "  3.843842973078188,\n",
       "  3.640671542603665,\n",
       "  3.638915065161101,\n",
       "  3.523677476462022,\n",
       "  3.5126245300155796,\n",
       "  3.5213122567628403,\n",
       "  3.40769160267778,\n",
       "  3.433688996419004,\n",
       "  3.4400530191841776,\n",
       "  3.4606514280150944,\n",
       "  3.400685881958208,\n",
       "  3.3805348365421533,\n",
       "  3.3593656835454584,\n",
       "  3.395032724609245,\n",
       "  3.379832645990905,\n",
       "  3.368972884729283,\n",
       "  3.3495622565795378,\n",
       "  3.3120032896088953,\n",
       "  3.3315032612296047,\n",
       "  3.3290950305100018,\n",
       "  3.326442702124116,\n",
       "  3.3294834582782475,\n",
       "  3.3203311183789292,\n",
       "  3.3219422245369303,\n",
       "  3.310937632791595,\n",
       "  3.326942328591947,\n",
       "  3.2946689130943265,\n",
       "  3.284719165687084,\n",
       "  3.2764853707447292,\n",
       "  3.2654369590295858,\n",
       "  3.2620049113183605,\n",
       "  3.2853576688407924,\n",
       "  3.269405570234117,\n",
       "  3.261976518060251,\n",
       "  3.2675134161751265,\n",
       "  3.2503733104994383,\n",
       "  3.252903597323003,\n",
       "  3.2476065093624853,\n",
       "  3.2417232711033237,\n",
       "  3.242496133859143,\n",
       "  3.2456310785107116,\n",
       "  3.2385303156626564,\n",
       "  3.237175358726553,\n",
       "  3.235567349074185,\n",
       "  3.2406626639154434,\n",
       "  3.241057228042287,\n",
       "  3.2391788513444673,\n",
       "  3.2436202515281636,\n",
       "  3.234018525600452,\n",
       "  3.23628863093848,\n",
       "  3.2300874434637232,\n",
       "  3.2358707819697687,\n",
       "  3.232699728915826,\n",
       "  3.228391401451904,\n",
       "  3.2270876457211894,\n",
       "  3.227885655287484,\n",
       "  3.233156788804468,\n",
       "  3.228971624162123,\n",
       "  3.2359032692395022,\n",
       "  3.2311306771588457,\n",
       "  3.229097788968671,\n",
       "  3.228878113585394,\n",
       "  3.2284072847274627,\n",
       "  3.227599043538941,\n",
       "  3.228855784384294],\n",
       " 'lr': [0.001,\n",
       "  0.00092611875,\n",
       "  0.0008576959,\n",
       "  0.0007943282,\n",
       "  0.0007356423,\n",
       "  0.0006812921,\n",
       "  0.00063095737,\n",
       "  0.0005843414,\n",
       "  0.00054116955,\n",
       "  0.0005011872,\n",
       "  0.0004641589,\n",
       "  0.00042986622,\n",
       "  0.00039810716,\n",
       "  0.0003686945,\n",
       "  0.0003414549,\n",
       "  0.00031622776,\n",
       "  0.00029286445,\n",
       "  0.00027122727,\n",
       "  0.00025118864,\n",
       "  0.00023263051,\n",
       "  0.00021544346,\n",
       "  0.00019952623,\n",
       "  0.00018478498,\n",
       "  0.00017113284,\n",
       "  0.00015848932,\n",
       "  0.00014677993,\n",
       "  0.00013593564,\n",
       "  0.00012589255,\n",
       "  0.000116591444,\n",
       "  0.00010797752,\n",
       "  1e-04,\n",
       "  9.2611874e-05,\n",
       "  8.576959e-05,\n",
       "  7.943282e-05,\n",
       "  7.3564224e-05,\n",
       "  6.812921e-05,\n",
       "  6.309574e-05,\n",
       "  5.843414e-05,\n",
       "  5.4116954e-05,\n",
       "  5.0118724e-05,\n",
       "  4.6415887e-05,\n",
       "  4.2986623e-05,\n",
       "  3.981072e-05,\n",
       "  3.686945e-05,\n",
       "  3.4145487e-05,\n",
       "  3.1622778e-05,\n",
       "  2.9286446e-05,\n",
       "  2.7122725e-05,\n",
       "  2.5118865e-05,\n",
       "  2.326305e-05,\n",
       "  2.1544347e-05,\n",
       "  1.9952624e-05,\n",
       "  1.8478499e-05,\n",
       "  1.7113283e-05,\n",
       "  1.5848931e-05,\n",
       "  1.4677993e-05,\n",
       "  1.3593564e-05,\n",
       "  1.2589254e-05,\n",
       "  1.1659144e-05,\n",
       "  1.0797751e-05,\n",
       "  1e-05,\n",
       "  9.261187e-06,\n",
       "  8.576959e-06,\n",
       "  7.943282e-06,\n",
       "  7.3564224e-06,\n",
       "  6.8129207e-06,\n",
       "  6.3095736e-06,\n",
       "  5.8434143e-06,\n",
       "  5.4116954e-06,\n",
       "  5.0118724e-06,\n",
       "  4.641589e-06,\n",
       "  4.298662e-06,\n",
       "  3.9810716e-06,\n",
       "  3.686945e-06,\n",
       "  3.414549e-06,\n",
       "  3.1622776e-06,\n",
       "  2.9286446e-06,\n",
       "  2.7122726e-06,\n",
       "  2.5118864e-06,\n",
       "  2.326305e-06,\n",
       "  2.1544347e-06,\n",
       "  1.9952622e-06]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle _thread.RLock objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7f0660b41487>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'good_neural_net'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "fname = 'good_neural_net'\n",
    "pickle.dump(model, open(fname, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/andy/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "good_neural_net/variables; Not a directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c64ca5894648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \"\"\"\n\u001b[1;32m   1007\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1008\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n\u001b[0;32m--> 115\u001b[0;31m                           signatures, options)\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(model, filepath, overwrite, include_optimizer, signatures, options)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# we use the default replica context here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_default_replica_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m       \u001b[0msave_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[1;32m    913\u001b[0m   \u001b[0;31m# the checkpoint, copy assets into the assets directory, and write out the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m   \u001b[0;31m# SavedModel proto itself.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m   \u001b[0mutils_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create_variables_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m   \u001b[0mobject_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m   builder_impl.copy_assets_to_destination_dir(asset_info.asset_filename_map,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/utils_impl.py\u001b[0m in \u001b[0;36mget_or_create_variables_dir\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    212\u001b[0m   \u001b[0mvariables_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_variables_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecursive_create_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mvariables_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mrecursive_create_dir\u001b[0;34m(dirname)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m   \"\"\"\n\u001b[0;32m--> 440\u001b[0;31m   \u001b[0mrecursive_create_dir_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mrecursive_create_dir_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m   \"\"\"\n\u001b[0;32m--> 455\u001b[0;31m   \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursivelyCreateDir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: good_neural_net/variables; Not a directory"
     ]
    }
   ],
   "source": [
    "model.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51890 samples, validate on 12973 samples\n",
      "Epoch 1/20\n",
      "51890/51890 [==============================] - 11s 219us/sample - loss: 3.5021 - val_loss: 3.7694\n",
      "Epoch 2/20\n",
      "51890/51890 [==============================] - 9s 177us/sample - loss: 2.8638 - val_loss: 3.3172\n",
      "Epoch 3/20\n",
      "51890/51890 [==============================] - 9s 173us/sample - loss: 2.7562 - val_loss: 2.9667\n",
      "Epoch 4/20\n",
      "51890/51890 [==============================] - 8s 153us/sample - loss: 2.6725 - val_loss: 3.0613\n",
      "Epoch 5/20\n",
      "51890/51890 [==============================] - 8s 160us/sample - loss: 2.6169 - val_loss: 2.9736\n",
      "Epoch 6/20\n",
      "51890/51890 [==============================] - 8s 156us/sample - loss: 2.5084 - val_loss: 2.7974\n",
      "Epoch 7/20\n",
      "51890/51890 [==============================] - 8s 156us/sample - loss: 2.4934 - val_loss: 2.5694\n",
      "Epoch 8/20\n",
      "51890/51890 [==============================] - 8s 155us/sample - loss: 2.4545 - val_loss: 2.5054\n",
      "Epoch 9/20\n",
      "51890/51890 [==============================] - 8s 151us/sample - loss: 2.4644 - val_loss: 2.8485\n",
      "Epoch 10/20\n",
      "51890/51890 [==============================] - 8s 145us/sample - loss: 2.4163 - val_loss: 2.3460\n",
      "Epoch 11/20\n",
      "51890/51890 [==============================] - 9s 164us/sample - loss: 2.3929 - val_loss: 2.5530\n",
      "Epoch 12/20\n",
      "51890/51890 [==============================] - 8s 145us/sample - loss: 2.3446 - val_loss: 2.9199\n",
      "Epoch 13/20\n",
      "51890/51890 [==============================] - 7s 144us/sample - loss: 2.4444 - val_loss: 2.7757\n",
      "Epoch 14/20\n",
      "51890/51890 [==============================] - 8s 156us/sample - loss: 2.6114 - val_loss: 2.6884\n",
      "Epoch 15/20\n",
      "51890/51890 [==============================] - 8s 154us/sample - loss: 2.5307 - val_loss: 2.5432\n",
      "Epoch 16/20\n",
      "51890/51890 [==============================] - 8s 146us/sample - loss: 2.5560 - val_loss: 2.4435\n",
      "Epoch 17/20\n",
      "51890/51890 [==============================] - 7s 141us/sample - loss: 2.5084 - val_loss: 2.3199\n",
      "Epoch 18/20\n",
      "51890/51890 [==============================] - 8s 147us/sample - loss: 2.3974 - val_loss: 4.6785\n",
      "Epoch 19/20\n",
      "51890/51890 [==============================] - 8s 148us/sample - loss: 2.3418 - val_loss: 4.4332\n",
      "Epoch 20/20\n",
      "51890/51890 [==============================] - 8s 145us/sample - loss: 2.2984 - val_loss: 3.9390\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(19, activation='relu', input_shape=X_train_scaled.shape[1:]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for i in range(17, 1, -2):\n",
    "    model.add(keras.layers.Dense(i, activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam())\n",
    "history = model.fit(X_train_scaled, y_train, epochs=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.429\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('RMSE: %.3f' %rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2203a517bd9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# defaults are lr = 0.001, beta1 = 0.9, beta2 = 0.999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mADAM_LR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mADAM_BETA1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mADAM_BETA2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# adjust the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# beta(1,2) correspond to the importance of (momentum, scale(?))\n",
    "# defaults are lr = 0.001, beta1 = 0.9, beta2 = 0.999\n",
    "optimizer = keras.optimizers.Adam(learning_rate=ADAM_LR, beta_1=ADAM_BETA1, beta_2=ADAM_BETA2)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "# adjust the learning rate\n",
    "def lr_fn(epoch, init=ADAM_LR, decay_time=LR_10FOLD_DECAY_TIME):\n",
    "    return(init * 10**(-epoch/decay_time))\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(lr_fn)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=N_EPOCHS, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "if USE_DROPOUT_REGULARIZATION:\n",
    "    model.add(keras.layers.Dropout(rate=DROPOUT_RATE, input_shape=X_train_scaled.shape[1:]))\n",
    "model.add(keras.layers.Dense(19, activation=ACTIVATOR, kernel_initializer=INITIALIZER))\n",
    "if USE_BATCH_NORMALIZATION:\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "for i in np.arange(19, 1, -18/NUM_NODES): # always start with 19 and end before 1\n",
    "    j = int(i)\n",
    "    model.add(keras.layers.Dropout(rate=DROPOUT_RATE))\n",
    "    model.add(keras.layers.Dense(j, activation=ACTIVATOR, kernel_initializer=INITIALIZER))\n",
    "    if USE_BATCH_NORMALIZATION:\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(1, kernel_initializer=INITIALIZER)) # the final, predicting layer\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(keras.optimizers.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(1, 10, 1.5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(19, 1, -18/NUM_NODES):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
